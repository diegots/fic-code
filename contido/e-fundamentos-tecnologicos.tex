%%%%                          %%%%
%%%% FUNDAMENTOS TECNOLÓGICOS %%%%
%%%%                          %%%%

\chapter{Fundamentos tecnológicos}
\label{chap:fundamentos-tecnologicos}

\lettrine{E}{ste} capítulo tiene por objetivo presentar las principales herramientas utilizadas para el desarrollo y la elaboración de la memoria. También se hace una reseña de las tecnologías seleccionadas como bases del proyecto.

\section{Herramientas de trabajo}

El ordenador utilizado consistió en un portátil HP con un procesador Intel i5 de segunda generación, 12 GB de RAM y sistema operativo Ubuntu 18.04. Para el desarrollo en Python se utilizó el IDE Eclipse con el plugin Pydev
\footnote{\url{https://www.pydev.org}}. Este plugin añade a Eclipse características como completado de código, refactorización o debugging. Soporta la versión actual del lenguaje, la versión 3. Los editores Atom y vim sirvieron para los scripts y el código en C de los escáneres y parsers. La memoria está elaborada en \LaTeX con la ayuda de TeXstudio. Se utilizó Zotero para organizar la bibliografía y el plugin Better BibTeX (BBT) 
\footnote{\url{https://retorque.re/zotero-better-bibtex}} para gestionar las claves de las referencias en el fichero fuente \LaTeX. Este complemento permite la inclusión directa desde Zotero a TeXstudio con lo que agiliza en gran medida la gestión de las citas. Los diagramas están hechos con yEd, la herramienta para creación de diagramas de yWorks 
\footnote{\url{https://www.yworks.com/products/yed}}. Dropbox Paper es un editor \acrlong{wysiwyg} que funciona en un navegador y tiene la facilidad de representar la notación Markdown con un diseño agradable. Fue utilizado para la organización general, borradores, notas, etc.

\section{Tecnologías}

\subsection{OpenCV}

\textbf{OpenCV} \cite{opencvTeam_oficialSite_main} es una librería de fuente abierta para visión por computador y aprendizaje máquina. Desde su nacimiento en el año 1999 acumula más de 2500 algoritmos y muchas más funciones que hacen uso de ellos. El objetivo del proyecto es proveer una infraestructura para facilitar la construcción de aplicaciones complejas de forma rápida. Es ampliamente utilizado en todo el mundo gracias a las facilidades de su licencia BSD. 

\begin{wrapfigure}{R}{0.3\textwidth}
    \centering
    \includegraphics[width=0.25\textwidth]{imaxes/e-fundamentos-tecnologicos/logo-opencv.png}
\end{wrapfigure}

Está escrita en C y C++ pero dispone además de interfaces de compatibilidad para varios lenguajes de programación como Java, Python o MATLAB. Se puede instalar en los principales sistemas operativos para ordenadores y también en Android. Los ámbitos de aplicación son múltiples, así como el número de empresas grandes y pequeñas que la explotan. Se pueden encontrar usos de OpenCV en sistemas de seguridad,para análisis de calidad en fábricas, imagen médica, robótica, por citar algunos. En lo que respecta a este proyecto, se emplea la versión optimizada de la Transformada de Hough por medio de la capa de compatibilidad Python \footnote{La documentación del API Python se encuentra en \url{https://docs.opencv.org/4.5.2/dd/d1a/group__imgproc__feature.html}.}. Gracias a que se puede utilizar desde Python, abre la puerta a la incorporación de una fase que mejore a la calidad de las imágenes, previa a los procesos de OCR.

\subsection{Flex}
\label{subsec:flex}

Flex y Bison son las versiones modernas de Lex y Yacc respectivamente. Yacc fue el primero en ser creado por Stephen C. Johnson de los Laboratorios Bell. En esa misma época Mike Lesk y Eric Schmidt crearon Lex en AT\&T. Ambos proyectos tuvieron éxito pero también tenían problemas: especialmente sus licencias de uso restrictivas y el escaso rendimiento.
En 1985 Bob Corbett creó la versión inicial del software en que Richard Stallman se basó para crear Bison. De manera parecida, en 1987, Vern Paxson adaptó una versión de Lex a C que llega hasta nuestros días como Fast Lexical Analyzer Generator. Bison está mantenido actualmente por la Free Software Foundation. Flex es también open source y se puede encontrar en Github.

Flex es un generador de analizadores léxicos. Esto quiere decir que la salida de Flex es un programa C que una vez compilado se puede utilizar para buscar patrones en un texto de entrada. Así mismo, Bison se utiliza para generar analizadores sintácticos y su salida también es otro programa C. Es habitual utilizar ambas herramientas trabajando combinadas. Si se hace así, se obtiene un único fuente C que una vez convertido en un programa ejecutable es capaz de realizar una primera fase de análisis léxico seguida del análisis sintáctico. Internamente, los tokens reconocidos por Flex son comunicados a Bison, junto con la información semántica, si existe. Flex y Bison pueden aplicarse en la construcción de compiladores e intérpretes pero también para cualquier problema donde deban buscarse patrones en la entrada o que la entrada consista en un lenguaje o instrucciones.

En su forma más básica, un programa Flex consiste en un listado de expresiones regulares asociadas a una acción. El escáner generado trabaja leyendo la entrada y buscando patrones en ella. En el Listado \ref{lst:ejemplo-programa-flex} se puede ver un programa básico de ejemplo que es capaz de contar los caracteres, palabras y líneas de un texto. Los programas Flex se dividen en tres secciones separadas por los símbolos \verb|%%|, tal como aparecen en las líneas 7 y 11. La primera sección, conocida como sección de definiciones, contiene código C global y se suele utilizar para definir variables y funciones. La segunda sección es la sección de reglas y contiene líneas formadas por expresiones regulares seguidas de las acciones asociadas. La tercera sección, o sección de código de usuario, sirve como punto de entrada al programa. No se utiliza cuando se trabaja con Bison porque es este último el encargado de ejecutar la función \verb|yylex()| que da comienzo al análisis léxico.

\begin{lstlisting}[language=C,caption={Ejemplo de programa Flex mínimo},label=lst:ejemplo-programa-flex]
/* Esto es un comentario */
%{
int letras = 0;
int palabras = 0;
int lineas = 0;
%}
%%
[a-zA-Z]+ { palabras++; letras+=strlen(yytext); }
\n        { letras++; lineas++; }
.         { letras++; }
%%
main(int argc, char **argv)
{
    yylex();
    printf("%8d%8d%8d\n", lines, words, chars);
}
\end{lstlisting}

\subsection{Bison}

Bison es un generador de analizadores sintácticos creados a partir de la descripción de una Gramática Independiente del Contexto en formato BNF simplificado. Para conocer en detalle el funcionamiento y capacidades de Bison se puede consultar el manual de la aplicación, elaborado por la Free Software Foundation
\cite{fsf_web_bisonManual}. También es especialmente interesante el libro \emph{flex \& bison} de John Levine \cite{levine_book_flexBison}.

\label{automatas-pila} Todos los autómatas que soportan Lenguajes Independientes del Contexto utilizan dos elementos para su funcionamiento. Una estructura de tipo pila y una tabla de transición de estados. Los analizadores funcionan tratando de asociar reglas sintácticas con los tokens recibidos. Si una regla tiene varios terminales y/o no terminales en su parte derecha, se intentará completar la regla. Para hacerlo, los tokens se van acumulando en la pila. Esta acción se conoce como \emph{shift} o desplazamiento. Si en un estado dado es posible completar una regla con los tokens actuales y/o alguno o algunos de la pila, se produce una reducción y los tokens seleccionados se cambian por el no terminal de la parte izquierda de la regla utilizada, desapareciendo de la pila. Para que la reducción se pueda aplicar, el orden de los terminales y no terminales debe ser igual al prefijado en la regla. Este procedimiento continua hasta se produce un error gramatical o bien se llega al axioma, momento en que la cadena es válida y ya no quedan más tokens por procesar.

Existen múltiples maneras de construir la tabla y dependiendo del algoritmo utilizado el parser resultante será más o menos eficiente a la hora de calcular las transiciones de estados. Bison soporta tres variantes, LALR(1), IELR(1) y LR(1) o LR canónico. Por defecto se utiliza LALR (\emph{Lookahead} LR), que es más eficiente que LR(1) en el uso de la memoria, pero no soporta la misma cantidad de gramáticas. IELR es un algoritmo presentado en el 2008 \cite{dennyMalloy_paper_IELRAlgorithm} como una mejora al trabajo previo \emph{Practical General Method for Constructing LR(k)} \cite{pager_paper_constructLRparsers}. Además, si la gramática es ambigua o no determinista es posible configurar Bison para que genere un parser LR generalizado (GLR). Para un analizador, una gramática es no determinista cuando no hay un número fijo de símbolos de anticipación que puedan ser utilizados para determinar la siguiente regla gramatical que se debe aplicar.

% TODO otros parsers

% TODO explicar como se utilizan

En el Listado \ref{lst:ejemplo-programa-bison} se presenta un pequeño ejemplo de la estructura de un fichero fuente Bison. El diseño, en tres secciones, es similar al encontrado en el ejemplo de Flex en el Listado \ref{lst:ejemplo-programa-flex}. La primera sección contiene código en lenguaje C que es copiado directamente en el analizador generado. Esta primera sección también se utiliza para declarar los nombres de los terminales y no terminales utilizados en la gramática y asignarles un tipo de dato. La sección intermedia contiene las reglas gramaticales y, al igual que en Flex, se pueden especificar acciones semánticas. Por ejemplo, en la línea 18, se calcula el valor de una suma a partir de los valores semánticos de los tokens recibidos. El valor calculado es devuelto al siguiente nodo del árbol.

\begin{lstlisting}[language=C,caption={Ejemplo de programa Bison \cite{mit_web_bisonExample}},label=lst:ejemplo-programa-bison]
%{
    /* Sección del prólogo para código C común */
%}

%token double  NUM
%type  double  exp

%% /* Sección de reglas gramaticales y acciones */
input:   /* empty */
       | input line
       ;

line:    '\n'
       | exp '\n'  { printf ("\t%.10g\n", $1); }
       ;
    
exp:     NUM         { $$ = $1;      }
       | exp exp '+' { $$ = $1 + $2; }
       ;
%%
    /* Sección epílogo: código C para inicializar el parser. Contiene la función main() */
\end{lstlisting}

\subsection{Docker}

Los contenedores son una tecnología dentro del ámbito de la virtualización y \textbf{Docker} es una plataforma basada en estándares abiertos que permite automatizar la gestión de aplicaciones utilizando para ello este concepto. De forma similar a la virtualización tradicional basada en \gls{hipervisores}, los contenedores hacen posible coexistir múltiples instancias aisladas de la misma o diferentes aplicaciones en un  equipo anfitrión, pero los contenedores se ejecutan en espacio de usuario, sobre el núcleo del Sistema Operativo con el que se comunican de forma directa mediante llamadas a su API. Por este motivo se suele conocer a esta tecnología como virtualización a nivel Sistema Operativo. Un ejemplo simple de contenedor es el entorno \verb|chroot| disponible en Linux. Los entornos chroot permiten definir un directorio como base de un espacio donde los procesos se ejecutan aisladamente del resto del sistema. 

\begin{wrapfigure}{L}{0.3\textwidth}
    \centering
    \includegraphics[width=0.25\textwidth]{imaxes/e-fundamentos-tecnologicos/logo-docker.png}
\end{wrapfigure}

Una crítica habitual sobre el uso de contenedores es no alcanzar el mismo nivel de aislamiento entre procesos que el ofrecido por un hipervisor y se los considera menos seguros por este motivo. Pese a ello, los contenedores son utilizados con éxito para este mismo objetivo. Una de las ventajas de los contenedores frente a la virtualización estándar es que, al no requerir la capa de emulación y hacer uso de las llamadas directas a los servicios del Sistema Operativo, tienen menos sobrecarga, y esto se traduce en un arranque extremadamente rápido. Al consumir menos recursos se alcanzan unas mayores densidades por servidor, reduciendo los costes.

Docker tiene por objetivos principales:
\begin{itemize}
    \item Eliminar las disparidades entre los entornos de desarrollo, testing y producción haciendo que no existan para el desarrollador. En otras palabras, Docker consigue aislar la aplicación de la infraestructura en la que debe ejecutarse.
    \item Reducir el tiempo necesario para completar un ciclo de desarrollo habitual desde que el código es escrito, y probado, se realiza la carga en el entorno de producción y los usuarios comienzan a utilizarlo.
    \item Fomentar la creación de arquitecturas basadas en servicios. Idealmente una aplicación en un contenedor se ocupa de una única tarea y establece comunicación con otros contenedores para satisfacer sus necesidades. De este modo se facilita la distribución y análisis de las aplicaciones.
\end{itemize}

Existe documentación y guías para comenzar con esta tecnología \cite{dockerInc_web_startGuides} o artículos para profundizar en sus características \cite{turnbull_book_dockerBook}.

% TODO Valorar si presentar la librería de generación de JSON en C
